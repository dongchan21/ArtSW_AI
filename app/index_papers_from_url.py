# # index_papers_from_url.py

# import json
# import os
# import re
# import requests
# from pathlib import Path
# from typing import List, Dict, Any

# # LangChain ë¬¸ì„œ ë¡œë”ì™€ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„í¬íŠ¸
# from langchain_community.document_loaders import PyMuPDFLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter

# # ê¸°ì¡´ ëª¨ë“ˆ ì¬ì‚¬ìš©
# from app.core import embeddings, vectorstore
# from app.core.vectorstore import VectorTuple
# from app.core.settings import get_settings

# # --- ì„¤ì • ---
# # ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ê°€ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
# PAPER_METADATA_PATH = Path(__file__).parent.parent / "data" / "papers_metadata.json"
# print(f"ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {PAPER_METADATA_PATH.resolve()}")
# # ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
# TEMP_DOC_DIR = "./temp_docs"

# # âš ï¸ ìµœì†Œ ì²­í¬ ê¸¸ì´ ì„¤ì • (50ì ë¯¸ë§Œì€ ìœ ì˜ë¯¸í•œ ë¬¸ë§¥ì´ ë¶€ì¡±í•˜ë‹¤ê³  íŒë‹¨í•˜ì—¬ ì œì™¸)
# MIN_CHUNK_LENGTH = 50

# # -----------------------------------------------------------------------------
# # ë…¸ì´ì¦ˆ íŒë‹¨ í—¬í¼ í•¨ìˆ˜
# # -----------------------------------------------------------------------------

# def is_structural_noise(text: str) -> bool:
#     """
#     í…ìŠ¤íŠ¸ê°€ ëª©ì°¨, í—¤ë”, í˜ì´ì§€ ë²ˆí˜¸, ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ íƒìƒ‰ ë©”ë‰´/ê´‘ê³ ì™€ ê°™ì€
#     êµ¬ì¡°ì  ë…¸ì´ì¦ˆì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
#     """
#     cleaned_text = text.strip()
#     if not cleaned_text:
#         return True

#     lines = cleaned_text.split('\n')
#     num_lines = len(lines)

#     # 1. (ê¸°ì¡´ ìœ ì§€) í…ìŠ¤íŠ¸ì— ì—°ì†ì ì¸ ì (....)ì´ë‚˜ ê³µë°±ì´ ë§ì€ì§€ í™•ì¸ (ë‹¨ìˆœ ëª©ì°¨ íŒ¨í„´)
#     if re.search(r'[\. \-]{10,}', cleaned_text) and len(cleaned_text) < 200:
#         return True

#     # --- ğŸš¨ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë© ë…¸ì´ì¦ˆ ê°ì§€ (ìƒˆë¡œ ì¶”ê°€/ê°•í™”ëœ ë¡œì§) ---
    
#     # 2. ğŸš¨ ê´‘ê³ /í”„ë¡œëª¨ì…˜ ë° ë§í¬ íŒ¨í„´ ê²€ì‚¬ (ê°€ì¥ í™•ì‹¤í•œ ë…¸ì´ì¦ˆ ìœ í˜•)
#     # ì´ëª¨ì§€(ğŸš€), í• ì¸, ë“±ë¡, ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ë“± ê´‘ê³ ì„± í‚¤ì›Œë“œ ê°ì§€
#     noise_patterns = [
#         r'(new courses|off|enroll now|subscribe|join|20%)', # ê´‘ê³ /ìƒì—…ì  ë¬¸êµ¬
#         r'(github|discord|a new tab|social media|guideabout|aboutabout)', # ë§í¬/ì†Œì…œ ë¯¸ë””ì–´/íƒìƒ‰
#         r'[\U0001F600-\U0001F6FF\U00002600-\U000027BF]+', # ì´ëª¨ì§€ í¬í•¨
#     ]
#     for pattern in noise_patterns:
#         if re.search(pattern, cleaned_text, re.IGNORECASE | re.DOTALL):
#             print(f"   -> ğŸ’¥ êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ê°ì§€ (ê´‘ê³ /ë§í¬): {cleaned_text[:30]}...")
#             return True

#     # 3. ğŸš¨ íƒìƒ‰ ë©”ë‰´/ì „ì²´ ëª©ì°¨ ë¸”ë¡ ê°ì§€ (ì¤„ ë°”ê¿ˆ ë° ì§§ì€ ë¼ì¸ ë¹„ìœ¨)
#     # ì²­í¬ê°€ 5ì¤„ ì´ìƒì´ê³ , 30ì ë¯¸ë§Œì˜ ì§§ì€ ë¼ì¸ì´ 60% ì´ìƒì¸ ê²½ìš°
#     short_line_count = sum(1 for line in lines if len(line.strip()) < 30 and line.strip() != '')
#     if num_lines >= 5 and (short_line_count / num_lines) >= 0.6: 
#         print(f"   -> ğŸ’¥ êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ê°ì§€ (ì§§ì€ ì¤„ ë°˜ë³µ): {cleaned_text[:30]}...")
#         return True

#     # --- ğŸ“š í•™ìˆ /ë‹¨ìˆœ ë…¸ì´ì¦ˆ ê°ì§€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---

#     # 4. (ê¸°ì¡´ ìœ ì§€) í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ì§§ê³ , ì˜¤ì§ ìˆ«ì, ë¡œë§ˆ ìˆ«ì, ëŒ€ë¬¸ì ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°
#     if len(cleaned_text) < 100:
#         # 'I.', 'II.', 'A.', 'B.', 'CHAPTER ONE' ê°™ì€ íŒ¨í„´ í•„í„°ë§
#         if re.fullmatch(r'[\s\dIVXLCDM\.\(\)]+', cleaned_text.upper().replace(' ', '')):
#             print("   -> êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ê°ì§€ (ë¡œë§ˆ ìˆ«ì/ëŒ€ë¬¸ì):", cleaned_text)
#             return True
#         # ì¤„ ë°”ê¿ˆê³¼ í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ë§Œ ìˆëŠ” ê²½ìš°
#         if re.match(r'^[\s\d]*$', cleaned_text):
#             print("   -> êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ê°ì§€ (í˜ì´ì§€ ë²ˆí˜¸):", cleaned_text)
#             return True
        
#     return False



# # ----------------------------------------------------------------------
# # 1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•µì‹¬ ë¡œì§)
# # ----------------------------------------------------------------------

# def download_and_parse_pdf(paper_metadata: Dict[str, Any]) -> List[str]:
#     """URLì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ìœ íš¨í•œ ì²­í¬ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
#     url = paper_metadata["url"]
#     doc_id = paper_metadata["id"]
#     temp_file_path = Path(TEMP_DOC_DIR) / f"{doc_id}.pdf"
    
#     # 1. ë””ë ‰í† ë¦¬ ìƒì„±
#     Path(TEMP_DOC_DIR).mkdir(exist_ok=True)

#     try:
#         # 2. PDF ë‹¤ìš´ë¡œë“œ
#         print(f"   -> {doc_id}: íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         response = requests.get(url, stream=True)
#         response.raise_for_status() # HTTP ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
#         with open(temp_file_path, 'wb') as f:
#             for chunk in response.iter_content(chunk_size=8192):
#                 f.write(chunk)

#         # 3. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì¬ê·€ì  ì²­í‚¹
#         print(f"   -> {doc_id}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ ì¤‘...")
#         loader = PyMuPDFLoader(str(temp_file_path))
#         documents = loader.load()
        
#         # ì¬ê·€ì  ì²­í‚¹ ì ìš©
#         text_splitter = RecursiveCharacterTextSplitter(
#             separators=["\n\n", "\n", ".", " "], 
#             chunk_size=750, 
#             chunk_overlap=150 
#         )
#         split_docs = text_splitter.split_documents(documents)
        
#         # 4. ì²­í¬ í•„í„°ë§ ë° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
#         filtered_chunk_texts = []
#         for doc in split_docs:
#             chunk_text = doc.page_content.strip()
            
#             # ğŸ’¡ 1ì°¨ í•„í„°ë§: êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ì œê±° (ê°•í™”ëœ ë¡œì§ ì ìš©)
#             if is_structural_noise(chunk_text):
#                 # print(f"   -> ê²½ê³ : êµ¬ì¡°ì  ë…¸ì´ì¦ˆ ì œì™¸ë¨ (ëª©ì°¨/ê´‘ê³  ì¶”ì •): {chunk_text[:30]}...")
#                 continue
            
#             # ğŸ’¡ 2ì°¨ í•„í„°ë§: ìµœì†Œ ê¸¸ì´ í™•ì¸
#             if len(chunk_text) >= MIN_CHUNK_LENGTH:
#                 filtered_chunk_texts.append(chunk_text)
#             else:
#                 # print(f"   -> ê²½ê³ : ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸ë¨ (ê¸¸ì´: {len(chunk_text)}): {chunk_text[:30]}...")
#                 continue
                
#         print(f"   -> {doc_id}: ì´ {len(split_docs)}ê°œ ì²­í¬ ì¤‘ {len(filtered_chunk_texts)}ê°œ ì €ì¥.")
#         return filtered_chunk_texts

#     except Exception as e:
#         print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({doc_id}): {e}")
#         return []

#     finally:
#         # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
#         if temp_file_path.exists():
#             try:
#                 os.remove(temp_file_path)
#             except OSError as e:
#                 print(f"âŒ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")




# # ----------------------------------------------------------------------
# # 2. ë©”ì¸ ìƒ‰ì¸ í•¨ìˆ˜
# # ----------------------------------------------------------------------

# async def index_papers_from_json():
    
#     # 1. JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ
#     if not Path(PAPER_METADATA_PATH).exists():
#         print(f"âŒ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PAPER_METADATA_PATH}")
#         return
    
#     with open(PAPER_METADATA_PATH, 'r', encoding='utf-8') as f:
#         paper_metadata_list = json.load(f)

#     all_vectors_to_upsert: List[VectorTuple] = []
    
#     print(f"ì´ {len(paper_metadata_list)}ê°œì˜ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìƒ‰ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
#     for metadata in paper_metadata_list:
#         paper_id = metadata["id"]
        
#         # 2. ë‹¤ìš´ë¡œë“œ ë° ì²­í‚¹
#         chunks = download_and_parse_pdf(metadata)
        
#         if not chunks:
#             continue
            
#         print(f"  -> {paper_id}: ìµœì¢… {len(chunks)}ê°œì˜ ì²­í¬ ì¤€ë¹„ ì™„ë£Œ.")

#         # 3. ì²­í¬ ì„ë² ë”©
#         vectors = embeddings.embed_texts(chunks) 

#         # 4. Upsert ë°ì´í„° ì¤€ë¹„ (JSONì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨)
#         for i, (chunk_text, vector) in enumerate(zip(chunks, vectors)):
#             vector_id = f"{paper_id}_{i}"
            
#             # JSONì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ Pineconeì— ì €ì¥
#             metadata_for_pinecone = metadata.copy()
#             metadata_for_pinecone["text"] = chunk_text # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©
#             metadata_for_pinecone["chunk_index"] = i   # ì²­í¬ ìˆœì„œ
            
#             # Pineconeì— ì €ì¥í•  ë°ì´í„° í¬ê¸° ì œí•œì„ ìœ„í•´ í° í•„ë“œëŠ” ì œì™¸í•  ìˆ˜ ìˆìŒ
#             metadata_for_pinecone.pop("notes", None) 
            
#             all_vectors_to_upsert.append((vector_id, vector, metadata_for_pinecone))
    
#     # 5. Pineconeì— ì—…ë¡œë“œ
#     BATCH_SIZE = 100 # í•œ ë²ˆì— ì—…ë¡œë“œí•  ë²¡í„°ì˜ ê°œìˆ˜ë¥¼ 100ê°œë¡œ ì œí•œ (ì•ˆì „ì„ ìœ„í•´ 50~100 ê¶Œì¥)
#     total_vectors = len(all_vectors_to_upsert)

#     print(f"\nì´ {total_vectors}ê°œì˜ ë²¡í„°ë¥¼ {BATCH_SIZE}ê°œì”© ë°°ì¹˜ë¡œ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")

#     for i in range(0, total_vectors, BATCH_SIZE):
#         batch = all_vectors_to_upsert[i:i + BATCH_SIZE]
        
#         # vectorstore.pyì˜ upsert_vectors í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì—…ë¡œë“œ
#         try:
#             vectorstore.upsert_vectors(batch)
#             print(f"âœ… ë°°ì¹˜ {i//BATCH_SIZE + 1} ({len(batch)}ê°œ) ì—…ë¡œë“œ ì™„ë£Œ.")
#         except Exception as e:
#             print(f"âŒ ë°°ì¹˜ {i//BATCH_SIZE + 1} ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ê±´ë„ˆë›°ì§€ ì•Šê³  ê³„ì† ì‹œë„í• ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
#             break # ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ë¼ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

#     print("âœ… ë…¼ë¬¸ ë°ì´í„° ìƒ‰ì¸ ì™„ë£Œ.")



# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(index_papers_from_json())

import json
import os
import re
from pathlib import Path
from typing import List, Dict, Any
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.core import embeddings, vectorstore
from app.core.vectorstore import VectorTuple

TEMP_DOC_DIR = "./temp_docs"
MIN_CHUNK_LENGTH = 80
PAPER_PATH = Path(__file__).parent.parent / "data" / "Prompt_Engineering.pdf"

# --- 1ï¸âƒ£ ì„¹ì…˜ ì œëª© íŒ¨í„´ ì‚¬ì „ ---
TECHNIQUE_KEYWORDS = {
    "zero-shot": "zero_shot",
    "few-shot": "few_shot",
    "chain-of-thought": "chain_of_thought",
    "react": "react",
    "reflection": "reflection",
    "role prompting": "role_prompting",
    "contextual prompting": "contextual_prompting",
    "knowledge generation": "knowledge_generation",
    "tree of thoughts": "tree_of_thoughts",
    "automatic prompt engineering": "ape"
}


# --- 2ï¸âƒ£ ë…¸ì´ì¦ˆ ê°ì§€ ---
def is_noise(text: str) -> bool:
    text = text.strip()
    if not text:
        return True
    if re.match(r"^(Prompt Engineering|Author:|February 2025|Table of contents)", text):
        return True
    if len(text) < 40 and re.match(r"^[\s\dIVXLCDM\.\-]+$", text):
        return True
    if text.count("\n") > 6 and sum(len(l) < 25 for l in text.splitlines()) / len(text.splitlines()) > 0.6:
        return True
    return False


# --- 3ï¸âƒ£ ë³¸ë¬¸ ì˜ë¯¸ íŒë‹¨ ---
def is_meaningful(text: str) -> bool:
    ratio_letters = sum(c.isalpha() for c in text) / max(1, len(text))
    if ratio_letters < 0.4:
        return False
    if text.count(".") + text.count("?") + text.count("!") < 1:
        return False
    return True


# --- 4ï¸âƒ£ PDF ë¡œë“œ ë° ì²­í‚¹ ---
def parse_pdf_to_chunks(pdf_path: str) -> List[Dict[str, Any]]:
    loader = PyMuPDFLoader(pdf_path)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=120,
        separators=["\n\n", "\n", ".", " "],
    )
    split_docs = splitter.split_documents(documents)

    chunks = []
    current_section = None
    for doc in split_docs:
        text = doc.page_content.strip()
        if is_noise(text):
            continue
        if not is_meaningful(text):
            continue

        # ì„¹ì…˜ ì œëª© ê°ì§€
        for k in TECHNIQUE_KEYWORDS.keys():
            if re.search(k, text, re.IGNORECASE):
                current_section = k
                break

        chunks.append({
            "text": text,
            "section": current_section or "general",
            "page": getattr(doc.metadata, "page", None)
        })

    print(f"âœ… ìœ íš¨ ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks


# --- 5ï¸âƒ£ ì¸ë±ì‹± ë©”ì¸ í•¨ìˆ˜ ---
async def index_prompt_engineering_pdf():
    chunks = parse_pdf_to_chunks(str(PAPER_PATH))
    all_vectors: List[VectorTuple] = []

    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    texts = [c["text"] for c in chunks]
    vectors = embeddings.embed_texts(texts)

    for i, (vector, chunk) in enumerate(zip(vectors, chunks)):
        section = chunk["section"]
        meta = {
            "title": "Prompt Engineering (Google Cloud, 2025)",
            "section": section,
            "technique": TECHNIQUE_KEYWORDS.get(section, "general"),
            "page": str(chunk["page"]) if chunk.get("page") is not None else "unknown",
            "text": chunk["text"]
        }
        all_vectors.append((f"pe2025_{i}", vector, meta))

    print(f"ğŸ“¤ ì´ {len(all_vectors)}ê°œ ë²¡í„° ì—…ë¡œë“œ ì¤‘...")
    BATCH_SIZE = 100
    for i in range(0, len(all_vectors), BATCH_SIZE):
        batch = all_vectors[i:i + BATCH_SIZE]
        vectorstore.upsert_vectors(batch)
        print(f"âœ… ë°°ì¹˜ {i//BATCH_SIZE + 1} ì™„ë£Œ.")

    print("ğŸ¯ Prompt Engineering PDF ì¸ë±ì‹± ì™„ë£Œ!")


if __name__ == "__main__":
    import asyncio
    asyncio.run(index_prompt_engineering_pdf())
